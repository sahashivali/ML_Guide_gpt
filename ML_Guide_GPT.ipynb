{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahashivali/ML_Guide_gpt/blob/main/ML_Guide_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvXdW59Rar6M"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.28.0\n",
        "!pip install pyTelegramBotAPI\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGyrHzVQazOv"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA0r318La3bL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel,AutoTokenizer, GPT2TokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "def process_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    qa_pairs = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "        qa_pairs.append(f\"Question: {question}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "def load_dataset(file_path, tokenizer):\n",
        "    qa_pairs = process_csv(file_path)\n",
        "    tokenized_dataset = tokenizer(qa_pairs, truncation=True,\n",
        "                                  padding='max_length', max_length=128,\n",
        "                                  return_tensors=\"pt\")\n",
        "    dataset = Dataset.from_dict(tokenized_dataset)\n",
        "    return dataset\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "train_dataset = load_dataset(\"/content/ml.csv\", tokenizer)\n",
        "valid_dataset = load_dataset(\"/content/validation.csv\", tokenizer)\n",
        "\n",
        "# Configure and train the model using the Trainer class\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    warmup_steps=0,\n",
        "    logging_dir=\"logs\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UxiZ3MybbE6"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_ML_gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7VBWnjbY7PKt"
      },
      "outputs": [],
      "source": [
        "import telebot\n",
        "from transformers import GPT2LMHeadModel, AutoTokenizer, GPT2TokenizerFast\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_ML_gpt2\")\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize Telegram bot with bot token\n",
        "API_TOKEN = \"\"\n",
        "bot = telebot.TeleBot(API_TOKEN)\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "    welcome_message = \"Welcome to your ML Guide! Please ask the question.\"\n",
        "    bot.send_message(message.chat.id, welcome_message)\n",
        "\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def answer_question(message):\n",
        "    question = message.text\n",
        "\n",
        "    # Generate the answer using the fine-tuned GPT-2 model\n",
        "    answer = ask_question(question, fine_tuned_model, tokenizer)\n",
        "\n",
        "    # Send the answer back to the user\n",
        "    bot.send_message(message.chat.id, answer)\n",
        "\n",
        "def ask_question(question, model, tokenizer, max_length=150, num_return_sequences=1):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        temperature=1.0,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.replace(prompt, \"\").strip()\n",
        "\n",
        "    # Truncate the answer after the first newline character\n",
        "    answer = answer.split(\"\\n\")[0]\n",
        "    return answer\n",
        "\n",
        "# Start the bot\n",
        "bot.polling()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLz29Los12OaQ5Hljp0wTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}